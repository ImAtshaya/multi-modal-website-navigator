# ğŸ¯ Multi-Modal Website Navigator Using Voice, Text, and Gesture Recognition

![Python](https://img.shields.io/badge/Python-3.11-blue) ![Tkinter](https://img.shields.io/badge/Tkinter-GUI-green) ![OpenCV](https://img.shields.io/badge/OpenCV-4.x-orange) ![MediaPipe](https://img.shields.io/badge/MediaPipe-Hand%20Gestures-red) ![TensorFlow Lite](https://img.shields.io/badge/TensorFlow%20Lite-ML-purple)

---

## ğŸ§  Project Overview
This project is an **intelligent system** that allows users to interact with and navigate websites using **three modes of input**:

1. **Voice commands**  
2. **Text input**  
3. **Hand gestures**  

It enhances accessibility and provides a hands-free experience by integrating **speech recognition**, **computer vision**, and **GUI-based navigation** using **Python**.

---

## ğŸ› ï¸ Technologies & Tools Used
- **Programming Language:** Python  
- **GUI Framework:** Tkinter  
- **Voice Recognition:** `speech_recognition`, `pyaudio`  
- **Text Input:** Tkinter Input Box  
- **Gesture Recognition:** MediaPipe, OpenCV  
- **Web Automation:** `webbrowser` module  
- **ML Framework:** TensorFlow Lite (for model inference)  
- **IDE:** VS Code / PyCharm  
- **Operating System:** Windows

---

## ğŸ§© Modules and Components

### Voice Recognition Module
- Listens for voice input using a microphone.  
- Converts speech to text.  
- Navigates to the relevant website.  

### Text Command Module
- Accepts typed website names.  
- Opens the specified site via the browser.  

### Gesture Recognition Module
- Uses webcam to detect hand gestures.  
- Maps gestures to specific websites:
  - ğŸ‘ Thumbs Up â†’ YouTube  
  - âœŠ Fist â†’ Google  
  - âœŒï¸ Peace â†’ GitHub  
  - ğŸ–ï¸ Open Hand â†’ Wikipedia  

### Main GUI Interface
- Centralized control panel with buttons to activate each feature.  
- Built using Tkinter for a simple, interactive interface.

---

## ğŸ§ª Implementation Flow
1. User launches the main GUI (`main_gui.py`).  
2. Three options appear: Text, Voice, Gesture.  
3. Based on input mode:
   - **Text input:** Website is opened.  
   - **Voice input:** Converts speech to text and opens the site.  
   - **Gesture input:** Webcam recognizes hand gestures and opens websites.  
4. Feedback and errors are shown via GUI messages or print statements.

---

## ğŸŒŸ Key Features
- Multi-modal control (Text, Voice, Gesture)  
- Hands-free navigation  
- Real-time gesture recognition using webcam  
- Scalable for more gestures and commands  
- User-friendly GUI

---

## âœ… Installation
-Create virtual environment
python -m venv venv
-Activate environment
-Windows
venv\Scripts\activate
-Linux/Mac
source venv/bin/activate
-Install dependencies
pip install -r requirements.txt
-Dependencies example:
opencv-python, mediapipe, SpeechRecognition, pyaudio, tensorflow or tflite-runtime, tkinter.

---

## ğŸƒ Run the App
-Main GUI
python main_gui.py
-For gesture debugging
python gesture_module.py
-For voice testing
python voice_module.py

---

##ğŸ“‚ Project Structure
-â”œâ”€ README.md
-â”œâ”€ requirements.txt
-â”œâ”€ main_gui.py
-â”œâ”€ modules/
-â”‚  â”œâ”€ voice_module.py
-â”‚  â”œâ”€ text_module.py
-â”‚  â”œâ”€ gesture_module.py
-â”‚  â””â”€ utils.py
-â””â”€ assets/
-  â””â”€ icons/

---

##âš¡ Gesture Accuracy â€” Tips

-Ensure good lighting and high contrast.
-Keep camera at eye/shoulder level; avoid backlight.
-Adjust MediaPipe min_detection_confidence / min_tracking_confidence.
-Apply smoothing (require N consecutive frames before triggering).
-Optional calibration for user-specific gestures.

---

#ğŸ‘¤ Author
Atshaya  
https://github.com/ImAtshaya  

